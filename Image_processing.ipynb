{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image_processing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9dDRzql38R2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "5feb9644-e276-491f-9407-62e3395bfce0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fbaab5ab0f3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfunc_hair\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msmplextract_hair\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjaccard_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'func_hair'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import cv2 \n",
        "import numpy as np\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "from func_hair import smplextract_hair\n",
        "from PIL import Image\n",
        "from sklearn.metrics import jaccard_score\n",
        "from skimage.feature import graycomatrix, graycoprops\n",
        "import pyfeats\n",
        "from pathlib import Path\n",
        "import math\n",
        "\n",
        "pathFolder_train='D:/Respaldo/Documents/MAIA/UNICAS/AImageP/project/FILES/Dataset/train/' \n",
        "train_Array = [x for x in os.listdir(pathFolder_train) if os.path.isfile(os.path.join(pathFolder_train,x))]\n",
        "pathFolder_test='D:/Respaldo/Documents/MAIA/UNICAS/AImageP/project/FILES/Dataset/test/' \n",
        "test_Array = [x for x in os.listdir(pathFolder_test) if os.path.isfile(os.path.join(pathFolder_test,x))]\n",
        "pathFolder_GTtrain='D:/Respaldo/Documents/MAIA/UNICAS/AImageP/project/FILES/Dataset/GT_train/'\n",
        "GTtrain_Array = [x for x in os.listdir(pathFolder_GTtrain) if os.path.isfile(os.path.join(pathFolder_GTtrain,x))]\n",
        "outputFolder_path=\"D:/Respaldo/Documents/MAIA/UNICAS/AImageP/project/FILES/Dataset/Seg_train\"\n",
        "pathFolder_whHair_train=\"D:/Respaldo/Documents/MAIA/UNICAS/AImageP/project/FILES/Dataset/whHair_train/\"\n",
        "pathFolder_masks_train=\"D:/Respaldo/Documents/MAIA/UNICAS/AImageP/project/FILES/Dataset/seg_res_train/\"\n",
        "pathFolder_gtres_train=\"D:/Respaldo/Documents/MAIA/UNICAS/AImageP/project/FILES/Dataset/gt_res_train/\"\n",
        "\n",
        "#\n",
        "def remove_fov(seg_im):\n",
        "\n",
        "    seg_im=cv2.cvtColor(seg_im,cv2.COLOR_RGB2GRAY)\n",
        "    ret,thresh = cv2.threshold(seg_im,127,255,0)\n",
        "    x,y = (find_center(seg_im))\n",
        "\n",
        "# find contours in the binary image\n",
        "    contours,hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    Distances = []\n",
        "    for c in contours:\n",
        "   # calculate moments for each contour\n",
        "        M = cv2.moments(c)\n",
        "   # calculate x,y coordinate of center\n",
        "        cX = int(M[\"m10\"] / (M[\"m00\"] + 0.001))\n",
        "        cY = int(M[\"m01\"] / (M[\"m00\"]+ 0.001))\n",
        "        Distances.append(math.dist([x, y], [cX, cY]))\n",
        "   ##print(cX,cY)\n",
        "\n",
        "    #print(Distances)\n",
        "    index_min = np.argmin(Distances, axis=0)\n",
        "    cnt = contours[index_min]\n",
        "\n",
        "    blank = np.zeros(thresh.shape, dtype='uint8')\n",
        " \n",
        "    cv2.drawContours(blank, [cnt], -1, (255),cv2.FILLED)\n",
        "    \n",
        "\n",
        "    M = cv2.moments(cnt)\n",
        "    # calculate x,y coordinate of center\n",
        "    c_x = int(M[\"m10\"] / (M[\"m00\"] + 0.001))\n",
        "    c_y = int(M[\"m01\"] / (M[\"m00\"]+ 0.001))\n",
        "    #print(c_x,c_y)\n",
        "    #cv2.imshow(\"Image\", blank)\n",
        "    #cv2.waitKey(500)\n",
        "\n",
        "    return blank\n",
        "\n",
        "def find_center(image):\n",
        "    (h, w) = image.shape[:2] #w:image-width and h:image-height\n",
        "    (cx,cy)=(w//2, h//2)\n",
        "    return cx,cy\n",
        "\n",
        "def find_center_im(image):\n",
        "    (h, w) = image.shape[:2] #w:image-width and h:image-height\n",
        "    (cx,cy)=cv2.circle(image, (w//2, h//2), 7, (255, 255, 255), -1) \n",
        "    cv2.putText(image, \"center\", (w//2- 20, h//2 - 20),\n",
        "\tcv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "\t# show the image\n",
        "    #cv2.imshow(\"Image\", image)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    cv2.waitKey(500)\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.show()\n",
        "    return cx,cy\n",
        "\n",
        "def crop_center(img,cropx,cropy):\n",
        "    y,x,c = img.shape\n",
        "    x\n",
        "    y\n",
        "    c\n",
        "    # startx = x//2 - cropx//2 round(cropx*0.75)\n",
        "    # starty = y//2 - cropy//2 \n",
        "    x1 = 200 \n",
        "    x2 = 900 \n",
        "    y1= 50\n",
        "    y2 = 750\n",
        "  \n",
        "    return img[y1:y2,x1:x2,:]\n",
        "    #img[starty:starty+cropy, startx:startx+cropx, :]\n",
        "\n",
        "def create_patch(imag1):\n",
        "\n",
        "    (h, w) = imag1.shape[:2]\n",
        "    x_coord_center= w//2 #imag1.shape[1]\n",
        "    y_coord_center= h//2 #imag1.shape[0]\n",
        "    patch_size = 40\n",
        "    image_patch=crop_center(imag1,x_coord_center, y_coord_center)\n",
        "    cv2.putText(imag1, \"center\", (w//2- 20, h//2 - 20),\n",
        "\t  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "    cv2.imshow(\"Image\", imag1)\n",
        "    cv2.waitKey(500)\n",
        "   \n",
        "    return image_patch\n",
        " \n",
        "def substract_2_im(im1,im2):\n",
        "    prep_img = im1 * im2\n",
        "    return prep_img\n",
        "\n",
        "def invert_image(imagem):\n",
        "    imagem = (255-imagem)\n",
        "    return imagem\n",
        "\n",
        "def segmentation(img_o):\n",
        "   \n",
        "    down_width = 817\n",
        "    down_height = 613\n",
        "    down_points = (down_width, down_height)\n",
        "    \n",
        "    img_r= extract_hair(resized_down,256,192)\n",
        "    img_r=img_o\n",
        "  \n",
        "\n",
        "    #apply median filter to remove noise \n",
        "    median = cv2.medianBlur(img_r,5) \n",
        "    #cv2.imshow(\"mediana\",median)\n",
        "    Z = median.reshape((-1,3))\n",
        "   \n",
        "   ############################################################################\n",
        "   ##############k-means#######################################################\n",
        "   #########################################################################\n",
        "   \n",
        "    #    convert to np.float32\n",
        "    Z = np.float32(Z)\n",
        "    # define criteria, number of clusters(K) and apply kmeans()\n",
        "    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
        "    K = 8 #default number for optimal results\n",
        "    flags=cv2.KMEANS_RANDOM_CENTERS # select initial cluster based on randomness\n",
        "    ret,label,center=cv2.kmeans(Z,K,None,criteria,10,flags)\n",
        "\n",
        "    # Now convert back into uint8, and make original image\n",
        "    center = np.uint8(center)\n",
        "    res = center[label.flatten()]\n",
        "    kmeans_img = res.reshape((img_r.shape))\n",
        "\n",
        "   \n",
        "    # =============================================================================\n",
        "    # =============================================================================\n",
        "    #    Adaptive histogram equalization  \n",
        "    # =============================================================================\n",
        "    clahe = cv2.createCLAHE(clipLimit=3., tileGridSize=(8,8))\n",
        "\n",
        "    hsv = cv2.cvtColor(kmeans_img, cv2.COLOR_BGR2HSV)# convert from BGR to HSV color space\n",
        "     \n",
        "    h, s, v = cv2.split(hsv)  # split on 3 different channels\n",
        "    #   apply CLAHE to the L-channel\n",
        "    h1 = clahe.apply(h)\n",
        "    s1 = clahe.apply(s)\n",
        "    v1 = clahe.apply(v)\n",
        "\n",
        "    lab = cv2.merge((h1,s1,v1))  # merge channels\n",
        "    #cv2.imshow(\"lab\",lab)\n",
        "  \n",
        "        \n",
        "    Enhance_img= cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)  # convert from LAB to BGR\n",
        "    \n",
        "  \n",
        "        \n",
        "    # =============================================================================\n",
        "    #    making the mask for grabcut\n",
        "    # =============================================================================\n",
        "    \n",
        "    \n",
        "    #Green color was extracted from the image and applying thresholding mask was obtained. \n",
        "    hsv = cv2.cvtColor(Enhance_img, cv2.COLOR_BGR2HSV)  \n",
        "    # function returns an array of elements equal to 0 if the elements of the given array do not lie between the two arrays representing the upper bounds and the lower bounds.\n",
        "    #where upper lower bounds are values of hsv   \n",
        "    lower_green = np.array([50,100,100])\n",
        "    upper_green = np.array([100,255,255])\n",
        "    mask_g = cv2.inRange(hsv, lower_green, upper_green)\n",
        "   \n",
        "    # The method returns two outputs. The first is the threshold that was used and the second output is the thresholded image\n",
        "    ret,inv_mask = cv2.threshold(mask_g,127,255,cv2.THRESH_BINARY_INV)\n",
        "   \n",
        "    #  \n",
        "    res = cv2.bitwise_and(img_r,img_r, mask= mask_g)\n",
        "    #mask haivng zeros \n",
        "    mask = np.zeros(img_r.shape[:2],np.uint8)\n",
        "    # Create 2 arrays for background and foreground model \n",
        "    bgdModel = np.zeros((1,65),np.float64)\n",
        "    fgdModel = np.zeros((1,65),np.float64)\n",
        "      \n",
        "    \n",
        "    \n",
        "  #####################################################################################\n",
        "    # =============================================================================\n",
        "    #      GRABCUT\n",
        "    # =============================================================================\n",
        "    #number sum decreased it showed better segmentation to avoid inversion we set lower number\n",
        "    #we used 70% of pixel value \n",
        "    if (np.sum(inv_mask[:])<8507940):\n",
        "        newmask = inv_mask\n",
        "    #exceeds threshold we use inverted masks \n",
        "\n",
        "    \n",
        "        # wherever it is marked white (sure foreground), change mask=1\n",
        "        # wherever it is marked black (sure background), change mask=0\n",
        "        mask[newmask == 0] = 0\n",
        "        mask[newmask == 255] = 1\n",
        "        #change pixel 0,2 to background and 1,3 to foreground \n",
        "        mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n",
        "        GrabCut_img2 = img_r*mask2[:,:,np.newaxis]\n",
        "       \n",
        "    else:\n",
        "         #initializing the Rectangle based on the image dimention\n",
        "        s = (img_r.shape[0] / 10, img_r.shape[1] / 10)\n",
        "        rect = (int(s[0]), int(s[1]), int(img_r.shape[0]) -int((3/10)) * int(s[0]), int(img_r.shape[1]) - int(s[1]))\n",
        "        #applying grab cut applies rectangle around the image\n",
        "        cv2.grabCut(lab,mask,rect,bgdModel,fgdModel,10,cv2.GC_INIT_WITH_RECT)\n",
        "        mask2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n",
        "        GrabCut_img2= img_r*mask2[:,:,np.newaxis]\n",
        "      \n",
        "     \n",
        "\n",
        "    # =============================================================================\n",
        "    # Binarization\n",
        "    # =============================================================================\n",
        "    imgmask2 = cv2.medianBlur(GrabCut_img2,5)\n",
        "    ret,Segmented_mask2 = cv2.threshold(imgmask2,0,255,cv2.THRESH_BINARY)\n",
        "    cv2.waitKey(500)\n",
        "    plt.imshow(GrabCut_img2)\n",
        "    return Segmented_mask2\n",
        "  # ==================================================\n",
        "av_Jaccard = 0\n",
        "for i in range(0,200):#range(len(train_Array)): \n",
        "    wjpg=train_Array[i]\n",
        "    output= wjpg.split(\".\", 1)\n",
        "    id=output[0]\n",
        "    img_o=cv2.imread(\"D:/Respaldo/Documents/MAIA/UNICAS/AImageP/project/FILES/Dataset/200Jaccard/\"+id +'_wh.jpg') #PREPROCES WHAIR\n",
        "    #img_r=cv2.imread(pathFolder_masks_train+id+'_segmask.png')\n",
        "    #print((train_Array[i]))\n",
        "    print(id)\n",
        "    img_seg = segmentation(img_o)\n",
        "\n",
        "    ####################fos performed################################################################\n",
        "    #print(\"shapeimgdseg\",img_seg.shape)\n",
        "    # cv2.imwrite(os.path.join(outputFolder_path, train_Array[i]+'_segmask.png'),img_seg)\n",
        "    # img_seg[img_seg == 255] = 1\n",
        "    # mfos=img_o[img_seg]\n",
        "    # print(i)\n",
        "    # print(type(mfos))\n",
        "    # print(type(img_seg))\n",
        "    # print(\"shapemfos\",mfos.shape)\n",
        "    # cv2.imshow(\"seg\",img_seg)\n",
        "    # cv2.waitKey(0)\n",
        "    #img_rfv=remove_fov(img_seg)\n",
        "    #print('s',img_rfv.shape)\n",
        "    #img_rfv=cv2.cvtColor(img_rfv,cv2.COLOR_GRAY2RGB)\n",
        "    #print('ns',img_rfv.shape)\n",
        "    #cv2.imwrite(os.path.join(outputFolder_path,id+'_segmask.png'),img_seg)\n",
        "    #print(\"immseg\",img_rfv)\n",
        "    cv2.imshow(\"seg mask\",img_seg)\n",
        "    h1,w1,c1=img_seg.shape\n",
        "    if img_seg[int(h1/2),int(w1/2)].all()==0:\n",
        "       img_seg=255-img_seg\n",
        "    #cv2.imshow(\"final mask\",img_rfv)\n",
        "    #cv2.waitKey(500)\n",
        "    #img_seg[img_seg == 255] = 1\n",
        "    #img_rfv[img_rfv==255]=1\n",
        "    #substraction=substract_2_im(img_o,img_seg)\n",
        "    #substraction=substract_2_im(img_o,img_rfv)\n",
        "    #print(\"maskwimag\",substraction)\n",
        "    #cv2.imshow(\"subs\",substraction)\n",
        "    #cv2.waitKey(0)\n",
        "    #extraction=create_patch(img_o)\n",
        "    #cv2.imshow(\"ext\",extraction)\n",
        "    #cv2.waitKey(0)\n",
        "    img_gt=cv2.imread(pathFolder_GTtrain+GTtrain_Array[i])\n",
        "    cv2.imshow(\"gt\",img_gt)\n",
        "    #cv2.destroyAllWindows()\n",
        "   \n",
        "    resized_up=cv2.resize(img_seg, (img_gt.shape[1], img_gt.shape[0]))\n",
        "    print(resized_up.shape)\n",
        "    cv2.imshow(\"res up\",resized_up)\n",
        "    cv2.waitKey(100)\n",
        "    cv2.destroyAllWindows()\n",
        "    gt = (img_gt/255).astype(np.uint8)\n",
        "    seg = (img_seg/255).astype(np.uint8)\n",
        "    ################################jaccard###############################\n",
        "    # Jaccard :\n",
        "    J = jaccard_score(gt.ravel(), seg.ravel())\n",
        "   # J = jaccard_score(gt.flatten(), seg.flatten())\n",
        "    print(J)\n",
        "    av_Jaccard = av_Jaccard + J\n",
        "    print(av_Jaccard)\n",
        "\n",
        "final_Jacc = av_Jaccard/200 #len(train_Array) #/4\n",
        "print(final_Jacc)\n",
        "\n",
        "# #jaccard_score(gt,resized_up)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hair removal function**"
      ],
      "metadata": {
        "id": "groOcax4pzKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hair removal function\n",
        "\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import os\n",
        "from skimage.transform import resize\n",
        "\n",
        "\n",
        "#function that performs the sum of the structure elements in different directions\n",
        "def sum_operation_se_different_directions(img,operation, width,height,n_se):\n",
        "    # create SEs\n",
        "    base = np.zeros([width, width])\n",
        "    k = int(width / 2 - height / 2)\n",
        "    while k <= (width / 2 + height / 2):\n",
        "        base = cv.line(base, (0, k), (width, k), 255) #drawing a line\n",
        "        k = k + 1\n",
        "        #print(k)\n",
        "    SEs = []\n",
        "    SEs.append(base)\n",
        "    angle = 180.0 / n_se\n",
        "    #Aplying affine transformations to the elements \n",
        "    for k in range(1, n_se):\n",
        "        SEs.append(cv.warpAffine(base, cv.getRotationMatrix2D((base.shape[0] / 2, base.shape[1] / 2), k * angle, 1.0),(width, width)))\n",
        "    # cv.imshow(\"see\",SEs[0])\n",
        "    #print(SEs[2].shape)\n",
        "    #Performing the sum of the elements with a morphology operation, in this case blackhats\n",
        "    open_sum = np.uint16(0*cv.morphologyEx(img, operation, np.uint8(SEs[0])))\n",
        "    for se in SEs:\n",
        "        open_sum += cv.morphologyEx(img, operation, np.uint8(se))\n",
        "    #Applying the normalization\n",
        "    result= cv.normalize(open_sum, 0, 255, norm_type=cv.NORM_MINMAX)\n",
        "    return np.uint8(result)\n",
        "\n",
        "\n",
        "def smplextract_hair(src):\n",
        "    \n",
        "    # Convert the original image to grayscale\n",
        "    img_gray=cv.cvtColor(src,cv.COLOR_BGR2GRAY)\n",
        "    #Applying the contrast enhance to the image, setting a contrast limit to avoid noise amplification\n",
        "    clahe = cv.createCLAHE(clipLimit=5.0, tileGridSize=(8, 8))\n",
        "    contrast_enhanced_gray_img = clahe.apply(img_gray)\n",
        "\n",
        "    #Selecting the size and number of structure elements\n",
        "    width= 29\n",
        "    height=2\n",
        "    n_se=15\n",
        "    \n",
        "    #cv.imshow(\"original_image\",src)\n",
        "    #Performing the sum of black hats in order to obtain the contours of the hairs with respect to the direction\n",
        "    sum_black_hats=sum_operation_se_different_directions(contrast_enhanced_gray_img,cv.MORPH_BLACKHAT, width,height,n_se)\n",
        "    #cv.imshow(\"sum_blackhats\", sum_black_hats)\n",
        "    #Applying the threshold to increase the pixel intensities in the mask\n",
        "    ret, bin_img = cv.threshold(sum_black_hats, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
        "    #cv.imshow(\"Thresholded Mask\",bin_img)\n",
        "    #Applying dilation to prepare the mask for the next step\n",
        "    dilated_bin_img=cv.dilate(bin_img,cv.getStructuringElement(cv.MORPH_ELLIPSE,(3,3)))\n",
        "    #cv.imshow(\"dilated\", dilated_bin_img)\n",
        "    # Inpaint the original image depending on the mask\n",
        "    dst = cv.inpaint(src,dilated_bin_img,7,cv.INPAINT_TELEA)\n",
        "    #cv.imshow(\"Inpaint\",dst)\n",
        "    #cv.waitKey(50)\n",
        "    return dst"
      ],
      "metadata": {
        "id": "A_6MC74jc74u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}